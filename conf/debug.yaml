defaults:
  - _self_

# ================================
# Main Experiment Configs
# ================================
# env: visual-antmaze-medium-navigate-v0
env: visual-cube-single-play-v0
seed: 1
total_timesteps: 500000
device: cuda
data_folder: /home/toolkit/snowrepo/data/

# ================================
# Evaluation Configs
# ================================
eval_freq: 50000
log_freq: 5000
eval_eps: 5

# ================================
# Logging and Saving
# ================================
project_name: Debug_${env}_${seed}
wandb_project: OGWorld
eval_folder: ./evals
log_folder: ./logs
save_folder: ./checkpoint
results_folder: ./results
save_experiment: false
save_freq: 100000
load_experiment: false
debug: true

# ================================
# Agent Hyperparameters
# ================================
frame_stack: 0

hp:
  # Generic
  batched_buffer: false
  chunks: 3
  batch_size: 8
  buffer_size: 1000000
  gc_negative: true
  discount: 0.99
  target_update_freq: 250
  storage_cpu: false
  subgoal_step: 25
  comp_return: true

  encoder_module: 'impala'
  enc_version: 'MrHiAc'  # V2, V2R
  enc_horizon: 5
  Q_horizon: 1
  expectile: 0.7
  lengthnorm_fn: true

  goal_dim: 32
  pixel_augs: false

  bufferV2: true
  vg_cur: 0.2
  vg_rand: 0.3 
  vg_traj: 0.5
  ag_cur: 0.0
  ag_rand: 0.0 
  ag_traj: 1.0

  # Encoder
  dyn_weight: 1.0
  reward_weight: 0.5
  idyn_weight: 0.1
  hdyn_weight: 0.3 # in Lambda
  zs_dim: 512
  encoder_lr: 1e-4
  encoder_wd: 1e-4

  # Value Model
  value_lr: 3e-4
  value_wd: 1e-4
  
  # High Policy
  high_actor_lr: 3e-4
  high_actor_wd: 1e-4
  high_alpha: 3.0

  # Low Policy 
  low_actor_lr: 3e-4
  low_actor_wd: 1e-4
  low_alpha: 3.0

  state_dependent_std: false
  const_std: true

  two_hot_loss: false
  num_bins: 21

  act_chunk: 5
  n_layer: 2
  stop_goal_grad: false

hydra:
  run:
    dir: ${log_folder}/${env}/${now:%Y-%m-%d_%H-%M-%S}
  # You can add more Hydra configs here if needed
